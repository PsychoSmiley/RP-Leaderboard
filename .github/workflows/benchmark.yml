name: Run RP Benchmark

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Model name (e.g., meta-llama/llama-4-maverick:free)'
        required: true
        type: string
      judge_model:
        description: 'Judge model (default: anthropic/claude-sonnet-4.5)'
        required: false
        type: string
        default: 'anthropic/claude-sonnet-4.5'

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 60

    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install requests

      - name: Run benchmark
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          OPENAI_API_BASE: https://openrouter.ai/api/v1
          MODEL_NAME: ${{ inputs.model_name }}
          JUDGE_MODEL: ${{ inputs.judge_model }}
        run: python benchmark.py

      - name: Commit and push results
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add results.json
          git diff --staged --quiet || git commit -m "ðŸ¤– Add benchmark results: ${{ inputs.model_name }}"
          git push
